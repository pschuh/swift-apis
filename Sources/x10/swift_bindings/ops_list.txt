- def: "abs(input: Tensor) -> Tensor"
  x10_enum: at::aten::abs
  shape_fn: input
  lower_fn: BuildAbs

- def: "acos(input: Tensor) -> Tensor"
  x10_enum: at::aten::acos
  shape_fn: input
  lower_fn: xla::Acos

- def: "acosh(input: Tensor) -> Tensor"
  x10_enum: at::aten::acosh
  shape_fn: input
  lower_fn: xla::Acosh

- def: "add(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::add
  lower_fn: LowerBinaryOp<xla::Add>

- def: "all(input: Tensor, dims: [Int64], keep_reduced_dimensions: Bool) -> Tensor"
  x10_enum: at::aten::all
  extras: ["canonicalize dims input"]
  lower_fn: BuildAll

- def: "any(input: Tensor, dims: [Int64], keep_reduced_dimensions: Bool) -> Tensor"
  x10_enum: at::aten::any
  extras: ["canonicalize dims input"]
  lower_fn: BuildAny

- def: "argmax(input: Tensor, dim: Int64, keepdim: Bool) -> Tensor"
  x10_enum: at::aten::argmax
  extras: ["canonicalize dim input"]
  lower_fn: BuildArgMax
  result_dtype: Long

- def: "argmin(input: Tensor, dim: Int64, keepdim: Bool) -> Tensor"
  x10_enum: at::aten::argmin
  extras: ["canonicalize dim input"]
  lower_fn: BuildArgMin
  result_dtype: Long

- def: "asin(input: Tensor) -> Tensor"
  x10_enum: at::aten::asin
  shape_fn: input
  lower_fn: xla::Asin

- def: "asinh(input: Tensor) -> Tensor"
  x10_enum: at::aten::asinh
  shape_fn: input
  lower_fn: xla::Asinh

- def: "atan(input: Tensor) -> Tensor"
  x10_enum: at::aten::atan
  shape_fn: input
  lower_fn: xla::Atan

- def: "atanh(input: Tensor) -> Tensor"
  x10_enum: at::aten::atanh
  shape_fn: input
  lower_fn: xla::Atanh

- def: "cat(input: [Tensor], dim: Int64) -> Tensor"
  extras: ["canonicalize dim input CanonicalizeCat"]
  x10_enum: at::aten::cat
  lower_fn: BuildCat

- def: "ceil(input: Tensor) -> Tensor"
  x10_enum: at::aten::ceil
  shape_fn: input
  lower_fn: xla::Ceil

- def: "clamp(input: Tensor, min: Tensor, max: Tensor) -> Tensor"
  x10_enum: at::aten::clamp
  shape_fn: input
  lower_fn: LowerClamp

- def: "constant_pad_nd(input: Tensor, pad: [Int64], value: AnyScalar) -> Tensor"
  extras: ["canonicalize pad input CanonicalizePad"]
  x10_enum: at::aten::constant_pad_nd
  lower_fn: LowerPad

- def: "cos(input: Tensor) -> Tensor"
  x10_enum: at::aten::cos
  shape_fn: input
  lower_fn: xla::Cos

- def: "cosh(input: Tensor) -> Tensor"
  x10_enum: at::aten::cosh
  shape_fn: input
  lower_fn: xla::Cosh

- def: "cumprod(input: Tensor, dim: Int64, dtype: ScalarType?, exclusive: Bool, reverse: Bool) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::cumprod
  shape_fn: CumOpShapeFn
  lower_fn: LowerCumProd

- def: "cumsum(input: Tensor, dim: Int64, dtype: ScalarType?, exclusive: Bool, reverse: Bool) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::cumsum
  shape_fn: CumOpShapeFn
  lower_fn: LowerCumSum

- def: "diagonal_value(input: Tensor, offset: Int64, dim1: Int64, dim2: Int64) -> Tensor"
  extras: ["canonicalize dim1 input", "canonicalize dim2 input"]
  x10_enum: at::aten::diagonal
  lower_fn: BuildDiagonal

- def: "div(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::div
  lower_fn: LowerBinaryOp<xla::Div>

- def: "dynamic_slice(base: Tensor, start_indices: [Tensor], slice_shapes: [Int64]) -> Tensor"
  x10_enum: at::aten::xla_dynamic_slice
  lower_fn: xla::DynamicSlice

- def: "dynamic_update_slice(base: Tensor, update: Tensor, start_indices: [Tensor]) -> Tensor"
  x10_enum: at::aten::xla_dynamic_update_slice
  lower_fn: xla::DynamicUpdateSlice

- def: "eq(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::eq
  lower_fn: LowerBinaryOp<xla::Eq>
  result_dtype: Bool

- def: "exp(input: Tensor) -> Tensor"
  x10_enum: at::aten::exp
  shape_fn: input
  lower_fn: xla::Exp

- def: "expand(input: Tensor, dims: [Int64]) -> Tensor"
  x10_enum: at::aten::expand
  extras: ["canonicalize dims input CanonicalizeExpand"]
  lower_fn: BuildExpand

- def: "expm1(input: Tensor) -> Tensor"
  x10_enum: at::aten::expm1
  shape_fn: input
  lower_fn: xla::Expm1

- def: "flip(input: Tensor, dims: [Int64]) -> Tensor"
  x10_enum: at::aten::flip
  extras: ["canonicalize dims input CanonicalizeFlip"]
  shape_fn: input
  lower_fn: xla::Rev

- def: "floor(input: Tensor) -> Tensor"
  x10_enum: at::aten::floor
  shape_fn: input
  lower_fn: xla::Floor

- def: "gather(input: Tensor, indices: Tensor, start_dim: Int64) -> Tensor"
  x10_enum: at::aten::index
  lower_fn: CreateIndex

- def: "ge(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::ge
  lower_fn: LowerBinaryOp<xla::Ge>
  result_dtype: Bool

- def: "gt(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::gt
  lower_fn: LowerBinaryOp<xla::Gt>
  result_dtype: Bool

- def: "is_finite(input: Tensor) -> Tensor"
  x10_enum: at::aten::xla_is_finite
  lower_fn: xla::IsFinite
  shape_fn: input
  result_dtype: Bool

- def: "is_inf(input: Tensor) -> Tensor"
  x10_enum: at::aten::xla_is_inf
  lower_fn: xla::IsInf
  shape_fn: input
  result_dtype: Bool

- def: "is_nan(input: Tensor) -> Tensor"
  x10_enum: at::aten::xla_is_nan
  shape_fn: input
  lower_fn: xla::IsNan
  result_dtype: Bool

- def: "le(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::le
  lower_fn: LowerBinaryOp<xla::Le>
  result_dtype: Bool

- def: "log(input: Tensor) -> Tensor"
  x10_enum: at::aten::log
  shape_fn: input
  lower_fn: xla::Log

- def: "log1p(input: Tensor) -> Tensor"
  x10_enum: at::aten::log1p
  shape_fn: input
  lower_fn: xla::Log1p

- def: "log_softmax(input: Tensor, dim: Int64) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::log_softmax
  lower_fn: BuildLogSoftmax
  shape_fn: input

- def: "log_softmax_backward(grad_output: Tensor, output: Tensor, dim: Int64) -> Tensor"
  extras: ["canonicalize dim grad_output"]
  x10_enum: at::aten::_log_softmax_backward_data
  shape_fn: grad_output
  lower_fn: BuildLogSoftmaxGrad

- def: "logicalAnd(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::logical_and
  lower_fn: LowerBinaryOp<xla::And>

- def: "logical_cast(input: Tensor, dtype: ScalarType) -> Tensor"
  x10_enum: xla_symbols::cast
  shape_fn: ShapeLogicalCast
  lower_fn: LowerLogicalCast
  result_dtype: dtype

- def: "logicalNot(input: Tensor) -> Tensor"
  x10_enum: at::aten::bitwise_not
  shape_fn: input
  lower_fn: xla::Not

- def: "logicalOr(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::logical_or
  lower_fn: LowerBinaryOp<xla::Or>

- def: "lt(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::lt
  lower_fn: LowerBinaryOp<xla::Lt>
  result_dtype: Bool

- def: "matmul(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::matmul
  lower_fn: LowerBinaryValueOp<CreateMatMul>

- def: "max(input: Tensor, dim: Int64, keepdim: Bool) -> Tensor"
  x10_enum: at::aten::max
  extras: ["canonicalize dim input"]
  lower_fn: BuildMaxInDim

- def: "maximum(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::max
  lower_fn: LowerBinaryOp<xla::Max>

- def: "mean(input: Tensor, dims: [Int64], keep_reduced_dimensions: Bool, dtype: ScalarType?) -> Tensor"
  extras: ["canonicalize dims input"]
  x10_enum: at::aten::mean
  lower_fn: LowerMean
  result_dtype: dtype

- def: "min(input: Tensor, dim: Int64, keepdim: Bool) -> Tensor"
  x10_enum: at::aten::min
  extras: ["canonicalize dim input"]
  lower_fn: BuildMinInDim

- def: "minimum(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::min
  lower_fn: LowerBinaryOp<xla::Min>

- def: "mm(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::mm
  lower_fn: xla::Dot

- def: "mul(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::mul
  lower_fn: LowerBinaryOp<xla::Mul>

- def: "ne(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::ne
  lower_fn: LowerBinaryOp<xla::Ne>
  result_dtype: Bool

- def: "neg(input: Tensor) -> Tensor"
  x10_enum: at::aten::neg
  shape_fn: input
  lower_fn: xla::Neg

- def: "nll_loss(logits: Tensor, labels: Tensor, ignore_index: Int64) -> Tensor"
  x10_enum: at::aten::nll_loss
  lower_fn: LowerNllLoss

- def: "permute_value(input: Tensor, dims: [Int64]) -> Tensor"
  x10_enum: at::aten::permute
  lower_fn: xla::Transpose

- def: "physical_cast(input: Tensor, dtype: ScalarType) -> Tensor"
  x10_enum: xla_symbols::cast
  shape_fn: ShapeLogicalCast
  lower_fn: LowerLogicalCast

- def: "pow(input: Tensor, other: Tensor) -> Tensor"
  x10_enum: at::aten::pow
  lower_fn: xla::Pow

- def: "prod(input: Tensor, dims: [Int64], keep_reduced_dimensions: Bool, dtype: ScalarType?) -> Tensor"
  extras: ["canonicalize dims input"]
  x10_enum: at::aten::prod
  lower_fn: LowerProd
  result_dtype: dtype

- def: "qr(input: Tensor, some: Bool) -> (Tensor, Tensor)"
  x10_enum: at::aten::qr
  lower_fn: LowerQR

- def: "relu(input: Tensor) -> Tensor"
  x10_enum: at::aten::relu
  lower_fn: BuildRelu

- def: "rem(input: Tensor, other: Tensor) -> Tensor"
  x10_enum: at::aten::xla_rem
  lower_fn: xla::Rem

- def: "repeat(input: Tensor, repeats: [Int64]) -> Tensor"
  x10_enum: at::aten::repeat
  lower_fn: BuildRepeat

- def: "resize_value(input: Tensor, repeats: [Int64]) -> Tensor"
  x10_enum: at::aten::resize
  lower_fn: BuildResize

- def: "round_to_even(input: Tensor) -> Tensor"
  x10_enum: at::aten::round_to_even
  lower_fn: xla::RoundToEven
  shape_fn: input

- def: "rsqrt(input: Tensor) -> Tensor"
  x10_enum: at::aten::rsqrt
  shape_fn: input
  lower_fn: xla::Rsqrt

- def: "select(input: Tensor, dim: Int64, index: Int64) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::select
  lower_fn: LowerSelect

- def: "sigmoid(input: Tensor) -> Tensor"
  x10_enum: at::aten::sigmoid
  shape_fn: input
  lower_fn: BuildSigmoid

- def: "sign(input: Tensor) -> Tensor"
  x10_enum: at::aten::sign
  shape_fn: input
  lower_fn: BuildSign

- def: "sin(input: Tensor) -> Tensor"
  x10_enum: at::aten::sin
  shape_fn: input
  lower_fn: xla::Sin

- def: "sinh(input: Tensor) -> Tensor"
  x10_enum: at::aten::sinh
  shape_fn: input
  lower_fn: xla::Sinh

- def: "slice(input: Tensor, dim: Int64, start: Int64, end: Int64, stride: Int64) -> Tensor"
  x10_enum: at::aten::slice
  shape_fn: ShapeSlice
  lower_fn: LowerSlice

- def: "softmax(input: Tensor, dim: Int64) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::softmax
  lower_fn: BuildSoftmax
  shape_fn: input

- def: "sqrt(input: Tensor) -> Tensor"
  x10_enum: at::aten::sqrt
  shape_fn: input
  lower_fn: xla::Sqrt

- def: "squeeze(input: Tensor, dim: Int64) -> Tensor"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::squeeze
  lower_fn: LowerSqueeze

- def: "stack(input: [Tensor], dim: Int64) -> Tensor"
  extras: ["canonicalize dim input CanonicalizeStack"]
  x10_enum: at::aten::stack
  lower_fn: BuildStack

- def: "sub(lhs: Tensor, rhs: Tensor) -> Tensor"
  x10_enum: at::aten::sub
  lower_fn: LowerBinaryOp<xla::Sub>

- def: "sum(input: Tensor, dims: [Int64], keep_reduced_dimensions: Bool, dtype: ScalarType?) -> Tensor"
  extras: ["canonicalize dims input"]
  x10_enum: at::aten::sum
  lower_fn: LowerSum
  result_dtype: dtype

- def: "svd(input: Tensor, compute_uv: Bool, full_matrix: Bool) -> (Tensor, Tensor, Tensor)"
  x10_enum: at::aten::svd
  lower_fn: LowerSVD

- def: "tan(input: Tensor) -> Tensor"
  x10_enum: at::aten::tan
  shape_fn: input
  lower_fn: xla::Tan

- def: "tanh(input: Tensor) -> Tensor"
  x10_enum: at::aten::tanh
  shape_fn: input
  lower_fn: xla::Tanh

- def: "tf_Conv(input: Tensor, filter: Tensor, depthwise: Bool, strides: [Int64], padding: TFPadding, explicit_paddings: [Int64], data_format: TFDataFormat, dilations: [Int64]) -> Tensor"
  x10_enum: at::aten::tf_convolution
  lower_fn: BuildTfConv

- def: "tf_ConvBackpropFilter(input: Tensor, filter_sizes: [Int64], out_backprop: Tensor, depthwise: Bool, strides: [Int64], padding: TFPadding, explicit_paddings: [Int64], data_format: TFDataFormat, dilations: [Int64]) -> Tensor"
  x10_enum: at::aten::tf_conv_backprop_filter
  lower_fn: BuildTfConvBackpropFilter

- def: "tf_ConvBackpropInput(input_sizes: [Int64], filter: Tensor, out_backprop: Tensor, depthwise: Bool, strides: [Int64], padding: TFPadding, explicit_paddings: [Int64], data_format: TFDataFormat, dilations: [Int64]) -> Tensor"
  x10_enum: at::aten::tf_conv_backprop_input
  lower_fn: BuildTfConvBackpropInput

- def: "tf_MirrorPad(input: Tensor, padding: [Int64], mode: TFMirrorPadMode) -> Tensor"
  x10_enum: at::aten::tf_mirror_pad
  lower_fn: BuildMirrorPad

- def: "tf_MirrorPadGrad(grad_output: Tensor, input_size: [Int64], padding: [Int64], mode: TFMirrorPadMode) -> Tensor"
  x10_enum: at::aten::tf_mirror_pad_backward
  lower_fn: BuildMirrorPadBackward

- def: "tf_OneHot(indices: Tensor, on_value: Tensor, off_value: Tensor, depth: Int64, axis: Int64) -> Tensor"
  result_dtype: on_value
  x10_enum: at::aten::tf_one_hot
  lower_fn: BuildOneHot

- def: "tf_StatelessRandomUniform(shape: [Int64], seeds: Tensor, minvalue: Tensor, maxvalue: Tensor) -> Tensor"
  x10_enum: at::aten::tf_stateless_random_uniform
  extras: ["shape_fn shape", "needs_lowering_context"]
  lower_fn: LowerTfStatelessRandomUniform
  result_dtype: minvalue

- def: "tf_UnsortedSegmentSum(data: Tensor, indicies: Tensor, num_segments: Int64) -> Tensor"
  x10_enum: at::aten::tf_unsorted_segment_sum
  lower_fn: LowerTfUnsortedSegmentSum

- def: "threshold(input: Tensor, output: Tensor, threshold: Float, value: Float) -> Tensor"
  x10_enum: at::aten::threshold_backward
  shape_fn: input
  lower_fn: BuildThreshold

- def: "topk(input: Tensor, k: Int64, dim: Int64, largest: Bool) -> (Tensor, Tensor)"
  extras: ["canonicalize dim input"]
  x10_enum: at::aten::topk
  lower_fn: BuildTopK

- def: "truncated_normal(input: Tensor) -> Tensor"
  x10_enum: at::aten::xla_truncated_normal
  shape_fn: input
  lower_fn: tensorflow::TruncatedNormal

- def: "update_slice(input: Tensor, source: Tensor, base_indices: [Int64]) -> Tensor"
  x10_enum: xla_symbols::update_slice
  lower_fn: BuildUpdateSlice

- def: "where(condition: Tensor, input: Tensor, other: Tensor) -> Tensor"
  x10_enum: at::aten::where
  shape_fn: input
  lower_fn: LowerWhere

- def: "xla_pad(input: Tensor, padding_value: AnyScalar, padding_config: PaddingConfig) -> Tensor"
  x10_enum: at::aten::xla_pad
  lower_fn: LowerPad

- def: "xla_slice(input: Tensor, start_indices: [Int64], limit_indices: [Int64], strides: [Int64]) -> Tensor"
  x10_enum: at::aten::xla_slice
  lower_fn: xla::Slice
