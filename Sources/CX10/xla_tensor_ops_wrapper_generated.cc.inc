// Autogenerated by codegen.py. Do not modify.

namespace swift_xla {
namespace ir {
namespace ops {
namespace {

class Cumprod : public Node {
 public:
  Cumprod(const Value& input, xla::int64 dim, c10::optional<at::ScalarType> dtype, bool exclusive, bool reverse)
      : Node(ir::OpKind(at::aten::cumprod),
             {input}, CumOpShapeFn(input, dim, dtype, exclusive, reverse),
             /*num_outputs=*/1, xla::util::MHash(dim, dtype, exclusive, reverse)),
        dim_(dim),
        dtype_(dtype),
        exclusive_(exclusive),
        reverse_(reverse) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cumprod>(
        operands.at(0), dim_, dtype_, exclusive_, reverse_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerCumProd(
        loctx->GetOutputOp(operand(0)), dim_, dtype_, exclusive_, reverse_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "dtype", dtype_);
    OpFieldToString(ss, "exclusive", exclusive_);
    OpFieldToString(ss, "reverse", reverse_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  c10::optional<at::ScalarType> dtype_;
  bool exclusive_;
  bool reverse_;
};

class Cumsum : public Node {
 public:
  Cumsum(const Value& input, xla::int64 dim, c10::optional<at::ScalarType> dtype, bool exclusive, bool reverse)
      : Node(ir::OpKind(at::aten::cumsum),
             {input}, CumOpShapeFn(input, dim, dtype, exclusive, reverse),
             /*num_outputs=*/1, xla::util::MHash(dim, dtype, exclusive, reverse)),
        dim_(dim),
        dtype_(dtype),
        exclusive_(exclusive),
        reverse_(reverse) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cumsum>(
        operands.at(0), dim_, dtype_, exclusive_, reverse_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerCumSum(
        loctx->GetOutputOp(operand(0)), dim_, dtype_, exclusive_, reverse_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "dtype", dtype_);
    OpFieldToString(ss, "exclusive", exclusive_);
    OpFieldToString(ss, "reverse", reverse_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  c10::optional<at::ScalarType> dtype_;
  bool exclusive_;
  bool reverse_;
};

class LogSoftmaxBackward : public Node {
 public:
  LogSoftmaxBackward(const Value& grad_output, const Value& output, xla::int64 dim)
      : Node(ir::OpKind(at::aten::_log_softmax_backward_data),
             {grad_output, output}, grad_output.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(dim) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogSoftmaxBackward>(
        operands.at(0), operands.at(1), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildLogSoftmaxGrad(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

}  // namespace
}  // namespace ops
}  // namespace ir
}  // namespace swift_xla

OpaqueXLATensor* XLATensor_cumprod(OpaqueXLATensor* input, int64_t dim, Optional_XLAScalarType dtype, bool exclusive, bool reverse) {
  auto input_ir_value = input->GetIrValue();
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Cumprod>(input_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(dim, input_ir_value.shape().rank()), dtype.value(), exclusive, reverse)));
}

OpaqueXLATensor* XLATensor_cumsum(OpaqueXLATensor* input, int64_t dim, Optional_XLAScalarType dtype, bool exclusive, bool reverse) {
  auto input_ir_value = input->GetIrValue();
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Cumsum>(input_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(dim, input_ir_value.shape().rank()), dtype.value(), exclusive, reverse)));
}

OpaqueXLATensor* XLATensor_log_softmax_backward(OpaqueXLATensor* grad_output, OpaqueXLATensor* output, int64_t dim) {
  auto grad_output_ir_value = grad_output->GetIrValue();
  auto output_ir_value = output->GetIrValue();
  return new swift_xla::XLATensor(grad_output->CreateFrom(
      swift_xla::ir::MakeNode<swift_xla::ir::ops::LogSoftmaxBackward>(grad_output_ir_value, output_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(dim, grad_output_ir_value.shape().rank()))));
}
