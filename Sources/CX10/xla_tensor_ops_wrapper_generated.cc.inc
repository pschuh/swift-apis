// Autogenerated by codegen.py. Do not modify.
 
namespace swift_xla {
namespace ir {
namespace ops {
namespace {

class LogSoftmaxBackward : public Node {
 public:
  LogSoftmaxBackward(const Value& grad_output, const Value& output, xla::int64 dim)
      : Node(ir::OpKind(at::aten::_log_softmax_backward_data),
             {grad_output, output}, grad_output.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(dim) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogSoftmaxBackward>(
        operands.at(0), operands.at(1), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildLogSoftmaxGrad(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString() << ", dim=" << dim_;
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

}  // namespace
}  // namespace ops
}  // namespace ir
}  // namespace swift_xla

OpaqueXLATensor* XLATensor_log_softmax_backward(OpaqueXLATensor* grad_output, OpaqueXLATensor* output, int64_t dim) {
  auto grad_output_ir_value = grad_output->GetIrValue();
  auto output_ir_value = output->GetIrValue();
  return new swift_xla::XLATensor(grad_output->CreateFrom(
      swift_xla::ir::MakeNode<swift_xla::ir::ops::LogSoftmaxBackward>(grad_output_ir_value, output_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(dim, grad_output_ir_value.shape().rank()))));
}
